{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trabalho Prático 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tqdm\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from utils.utils import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento das Imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:44<00:00, 82.11s/it] \n"
     ]
    }
   ],
   "source": [
    "# Loading the training and testing data\n",
    "print(\"Pre-processing...\")\n",
    "(train_images, train_labels), (test_images, test_labels) = load_data()\n",
    "\n",
    "# Converting the training and testing images and labels to numpy arrays\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Scaling the values of the images pixels to 0-1 to make the computation easier for our model\n",
    "train_images, test_images = train_images / 255, test_images / 255\n",
    "\n",
    "# Randomizing the training data\n",
    "train_images, train_labels = shuffle(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluating the Decision Tree Classifier on the raw pixel intensities\n",
    "#print(\"Evaluating DT...\")\n",
    "classifier = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None)\n",
    "\n",
    "# print(\"Fitting...\")\n",
    "classifier.fit(train_images, train_labels)\n",
    "\n",
    "# print(\"Predicting...\")\n",
    "y_pred = classifier.predict(test_images)\n",
    "\n",
    "# print('Done.')\n",
    "\n",
    "# Using the confusion matrix to print the accuracy of those predictions\n",
    "cm = confusion_matrix(y_true=test_labels, y_pred=np.round(y_pred))\n",
    "# print(f'\\nAccuracy Score Confusion Matrix (DT): {(cm.trace() / cm.sum()) * 100}%')\n",
    "\n",
    "# print('\\nClassification Report:')\n",
    "# print(classification_report(test_labels, y_pred))\n",
    "# print(classifier.score(test_images, y_pred))\n",
    "\n",
    "scores = cross_val_score(classifier, train_images, train_labels, cv=10)\n",
    "print(f'Decisione Tree Cross-validation score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Training\" and evaluating the KNN Classifier on the raw pixel intensities\n",
    "#print(\"Evaluating KNN...\")\n",
    "model = KNeighborsClassifier(n_neighbors = 10, n_jobs=-1)\n",
    "\n",
    "#print(\"Fitting...\")\n",
    "model.fit(train_images, train_labels)\n",
    "\n",
    "#print(\"Predicting...\")\n",
    "y_pred = model.predict(test_images)\n",
    "\n",
    "#print('Done.')\n",
    "\n",
    "# Using the confusion matrix to print the accuracy of those predictions\n",
    "cm = confusion_matrix(y_true=test_labels,y_pred=np.round(y_pred))\n",
    "\n",
    "#print(f'\\nAccuracy Score based on Confusion Matrix (KNN): {(cm.trace()/cm.sum())*100}%')\n",
    "cmKNN=confusion_matrix(y_true=test_labels,y_pred=np.round(y_pred))\n",
    "\n",
    "#print('\\nClassification Report:')\n",
    "#print(classification_report(test_labels, y_pred))\n",
    "\n",
    "scores = cross_val_score(model, train_images, train_labels, cv=10)\n",
    "print(f'KNN Cross-validation score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAUSSIAN-NB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluating classifier on the raw pixel intensities\n",
    "# print(\"Evaluating GaussianNB...\")\n",
    "\n",
    "sc = StandardScaler()\n",
    "train_images = sc.fit_transform(train_images)\n",
    "test_images = sc.transform(test_images)\n",
    "\n",
    "classifier = GaussianNB()\n",
    "\n",
    "#print(\"Fitting...\")\n",
    "classifier.fit(train_images, train_labels)\n",
    "\n",
    "#print(\"Predicting...\")\n",
    "y_pred = classifier.predict(test_images)\n",
    "\n",
    "#print('Done.')\n",
    "\n",
    "# Using the confusion matrix to print the accuracy of those predictions\n",
    "cm = confusion_matrix(y_true=test_labels,y_pred=np.round(y_pred))\n",
    "#print(f'\\nAccuracy Score Confusion Matrix (N-Bayes): {(cm.trace()/cm.sum())*100}%')\n",
    "\n",
    "#print('\\nClassification Report:')\n",
    "#print(classification_report(test_labels, y_pred))\n",
    "#print(f'Score: {classifier.score(test_images, y_pred)}')\n",
    "\n",
    "scores = cross_val_score(classifier, train_images, train_labels, cv=10)\n",
    "print(f'GaussianN Cross-validation score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SVM...\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluating the SVM Classifier on the raw pixel intensities\n",
    "# print(\"Evaluating SVM...\")\n",
    "classifier = SVC(kernel='rbf')  # Creating a SVM Classifier\n",
    "\n",
    "# print(\"Fitting...\")\n",
    "classifier.fit(train_images, train_labels)  # Model training with training set\n",
    "\n",
    "# print(\"Predicting...\")\n",
    "y_pred = classifier.predict(test_images) # Model predicting\n",
    "\n",
    "# print('Done.')\n",
    "\n",
    "# Using the confusion matrix to print the accuracy of those predictions\n",
    "cm = confusion_matrix(y_true=test_labels,y_pred=np.round(y_pred))\n",
    "# print(f'\\nAccuracy Score Confusion Matrix (SVM): {(cm.trace()/cm.sum())*100}%')\n",
    "\n",
    "# print('\\nClassification report: ')\n",
    "# print(classification_report(test_labels, y_pred))\n",
    "\n",
    "scores = cross_val_score(classifier, train_images, train_labels, cv=10)\n",
    "print(f'SVM Cross-validation score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from utils.utils import load_data_tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the image size, epochs, classes and batch size\n",
    "img = 150\n",
    "names = ['O', 'R']\n",
    "encode_name = {name: i for i, name in enumerate(names)}\n",
    "\n",
    "# Loading the training and testing data\n",
    "# print(\"Pre-processing...\")\n",
    "(train_images, train_labels), (test_images, test_labels) = load_data_tensorflow()\n",
    "\n",
    "# Converting the training and testing images and labels to numpy arrays\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Scaling the values of the images pixels to 0-1 to make the computation easier for our model\n",
    "train_images, test_images = train_images / 255, test_images / 255\n",
    "\n",
    "# Randomizing the training data\n",
    "train_images, train_labels = shuffle(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Evaluating TF model...\")\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(filters=32, activation='relu', input_shape=(img, img, 3), padding='same', kernel_size=(3, 3)),\n",
    "    Conv2D(filters=32, activation='relu', padding='same', kernel_size=(3, 3)),\n",
    "    MaxPool2D(pool_size=(2, 2)),\n",
    "    Conv2D(filters=64, activation='relu', padding='same', kernel_size=(3, 3)),\n",
    "    Conv2D(filters=64, activation='relu', padding='same', kernel_size=(3, 3)),\n",
    "    MaxPool2D(pool_size=(2, 2)),\n",
    "    Conv2D(filters=128, activation='relu', padding='same', kernel_size=(3, 3)),\n",
    "    Conv2D(filters=128, activation='relu', padding='same', kernel_size=(3, 3)),\n",
    "    MaxPool2D(pool_size=(2, 2)),\n",
    "    Conv2D(filters=256, activation='relu', padding='same', kernel_size=(3, 3)),\n",
    "    Conv2D(filters=256, activation='relu', padding='same', kernel_size=(3, 3)),\n",
    "    MaxPool2D(pool_size=(2, 2)),\n",
    "    Conv2D(filters=256, activation='relu', padding='same', kernel_size=(3, 3)),\n",
    "    Conv2D(filters=256, activation='relu', padding='same', kernel_size=(3, 3)),\n",
    "    MaxPool2D(pool_size=(2, 2)),\n",
    "    Conv2D(filters=512, activation='relu', padding='same', kernel_size=(3, 3)),\n",
    "    Conv2D(filters=512, activation='relu', padding='same', kernel_size=(3, 3)),\n",
    "    MaxPool2D(pool_size=(2, 2)),\n",
    "    Conv2D(filters=512, activation='relu', padding='same', kernel_size=(3, 3)),\n",
    "    Conv2D(filters=512, activation='relu', padding='same', kernel_size=(3, 3)),\n",
    "    Flatten(),\n",
    "    Dense(units=4096, activation='relu'),\n",
    "    Dense(units=4096, activation='relu'),\n",
    "    Dense(units=1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "# Printing the model summary\n",
    "model.summary()\n",
    "\n",
    "# Saving the weights of the model\n",
    "model.save('model.h5')\n",
    "\n",
    "# Compiling the model. Specifying the optimizer to act on the data. \n",
    "# The loss function (which could also have been sparse_categorical_crossentropy),\n",
    "# since there are only two classes, I have used binary_crossentropy\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Predicting...\")\n",
    "y_pred = model.predict(test_images)\n",
    "\n",
    "# print('Done.')\n",
    "\n",
    "# Using the confusion matrix to print the accuracy of those predictions\n",
    "cm = confusion_matrix(y_true=test_labels, y_pred=np.round(y_pred))\n",
    "# print(f'\\nAccuracy Score based on Confusion Matrix (TF): {(cm.trace() / cm.sum()) * 100}%')\n",
    "\n",
    "scores = cross_val_score(model, train_images, train_labels, cv=10)\n",
    "print(f'TensorFlow Cross-validation score: {scores.mean()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}