{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Trabalho Prático 1"
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\r\n",
    "import numpy as np\r\n",
    "import cv2\r\n",
    "import tqdm\r\n",
    "import random\r\n",
    "from sklearn.utils import shuffle\r\n",
    "from sklearn.metrics import classification_report, confusion_matrix\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn import tree\r\n",
    "from utils.utils import load_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pré-processamento das Imagens"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Loading the training and testing data\r\n",
    "print(\"Pre-processing...\")\r\n",
    "(train_images, train_labels), (test_images, test_labels) = load_data()\r\n",
    "\r\n",
    "# Converting the training and testing images and labels to numpy arrays\r\n",
    "train_images = np.array(train_images)\r\n",
    "train_labels = np.array(train_labels)\r\n",
    "\r\n",
    "test_images = np.array(test_images)\r\n",
    "test_labels = np.array(test_labels)\r\n",
    "\r\n",
    "# Scaling the values of the images pixels to 0-1 to make the computation easier for our model\r\n",
    "train_images, test_images = train_images / 255, test_images / 255\r\n",
    "\r\n",
    "# Randomizing the training data\r\n",
    "train_images, train_labels = shuffle(train_images, train_labels)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pre-processing...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2/2 [02:44<00:00, 82.11s/it] \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decision Tree Classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Training and evaluating the Decision Tree Classifier on the raw pixel intensities\r\n",
    "#print(\"Evaluating DT...\")\r\n",
    "classifier = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None)\r\n",
    "\r\n",
    "# print(\"Fitting...\")\r\n",
    "classifier.fit(train_images, train_labels)\r\n",
    "\r\n",
    "# print(\"Predicting...\")\r\n",
    "y_pred = classifier.predict(test_images)\r\n",
    "\r\n",
    "# print('Done.')\r\n",
    "\r\n",
    "# Using the confusion matrix to print the accuracy of those predictions\r\n",
    "cm = confusion_matrix(y_true=test_labels, y_pred=np.round(y_pred))\r\n",
    "# print(f'\\nAccuracy Score Confusion Matrix (DT): {(cm.trace() / cm.sum()) * 100}%')\r\n",
    "\r\n",
    "# print('\\nClassification Report:')\r\n",
    "# print(classification_report(test_labels, y_pred))\r\n",
    "# print(classifier.score(test_images, y_pred))\r\n",
    "\r\n",
    "scores = cross_val_score(classifier, train_images, train_labels, cv=10)\r\n",
    "print(f'Decisione Tree Cross-validation score: {scores.mean()}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# \"Training\" and evaluating the KNN Classifier on the raw pixel intensities\r\n",
    "#print(\"Evaluating KNN...\")\r\n",
    "model = KNeighborsClassifier(n_neighbors = 10, n_jobs=-1)\r\n",
    "\r\n",
    "#print(\"Fitting...\")\r\n",
    "model.fit(train_images, train_labels)\r\n",
    "\r\n",
    "#print(\"Predicting...\")\r\n",
    "y_pred = model.predict(test_images)\r\n",
    "\r\n",
    "#print('Done.')\r\n",
    "\r\n",
    "# Using the confusion matrix to print the accuracy of those predictions\r\n",
    "cm = confusion_matrix(y_true=test_labels,y_pred=np.round(y_pred))\r\n",
    "\r\n",
    "#print(f'\\nAccuracy Score based on Confusion Matrix (KNN): {(cm.trace()/cm.sum())*100}%')\r\n",
    "cmKNN=confusion_matrix(y_true=test_labels,y_pred=np.round(y_pred))\r\n",
    "\r\n",
    "#print('\\nClassification Report:')\r\n",
    "#print(classification_report(test_labels, y_pred))\r\n",
    "\r\n",
    "scores = cross_val_score(model, train_images, train_labels, cv=10)\r\n",
    "print(f'KNN Cross-validation score: {scores.mean()}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GAUSSIAN-NB Classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.naive_bayes import GaussianNB"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Training and evaluating classifier on the raw pixel intensities\r\n",
    "# print(\"Evaluating GaussianNB...\")\r\n",
    "\r\n",
    "sc = StandardScaler()\r\n",
    "train_images = sc.fit_transform(train_images)\r\n",
    "test_images = sc.transform(test_images)\r\n",
    "\r\n",
    "classifier = GaussianNB()\r\n",
    "\r\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\r\n",
    "gs_NB = GridSearchCV(estimator=classifier, \r\n",
    "                 param_grid=params_NB, \r\n",
    "                 verbose=1, \r\n",
    "                 scoring='accuracy');\r\n",
    "\r\n",
    "#print(\"Fitting...\")\r\n",
    "gs_NB.fit(train_images, train_labels)\r\n",
    "\r\n",
    "#print(\"Predicting...\")\r\n",
    "y_pred = gs_NB.predict(test_images)\r\n",
    "\r\n",
    "#print('Done.')\r\n",
    "\r\n",
    "# Using the confusion matrix to print the accuracy of those predictions\r\n",
    "cm = confusion_matrix(y_true=test_labels,y_pred=np.round(y_pred))\r\n",
    "#print(f'\\nAccuracy Score Confusion Matrix (N-Bayes): {(cm.trace()/cm.sum())*100}%')\r\n",
    "\r\n",
    "#print('\\nClassification Report:')\r\n",
    "#print(classification_report(test_labels, y_pred))\r\n",
    "#print(f'Score: {classifier.score(test_images, y_pred)}')\r\n",
    "\r\n",
    "scores = cross_val_score(gs_NB, train_images, train_labels, cv=10)\r\n",
    "print(f'GaussianN Cross-validation score: {scores.mean()}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SVM Classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from sklearn.svm import SVC"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Training and evaluating the SVM Classifier on the raw pixel intensities\r\n",
    "# print(\"Evaluating SVM...\")\r\n",
    "classifier = SVC(kernel='rbf', C=1.0, degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=None)  # Creating a SVM Classifier\r\n",
    "\r\n",
    "# print(\"Fitting...\")\r\n",
    "classifier.fit(train_images, train_labels)  # Model training with training set\r\n",
    "\r\n",
    "# print(\"Predicting...\")\r\n",
    "y_pred = classifier.predict(test_images) # Model predicting\r\n",
    "\r\n",
    "# print('Done.')\r\n",
    "\r\n",
    "# Using the confusion matrix to print the accuracy of those predictions\r\n",
    "cm = confusion_matrix(y_true=test_labels,y_pred=np.round(y_pred))\r\n",
    "# print(f'\\nAccuracy Score Confusion Matrix (SVM): {(cm.trace()/cm.sum())*100}%')\r\n",
    "\r\n",
    "# print('\\nClassification report: ')\r\n",
    "# print(classification_report(test_labels, y_pred))\r\n",
    "\r\n",
    "scores = cross_val_score(classifier, train_images, train_labels, cv=10)\r\n",
    "print(f'SVM Cross-validation score: {scores.mean()}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluating SVM...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TensorFlow "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.optimizers import Adam\r\n",
    "from sklearn.utils import shuffle\r\n",
    "from tensorflow.keras.layers import MaxPool2D\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\r\n",
    "from sklearn.metrics import confusion_matrix\r\n",
    "\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from utils.utils import load_data_tensorflow"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Setting the image size, epochs, classes and batch size\r\n",
    "img = 150\r\n",
    "names = ['O', 'R']\r\n",
    "encode_name = {name: i for i, name in enumerate(names)}\r\n",
    "\r\n",
    "# Loading the training and testing data\r\n",
    "# print(\"Pre-processing...\")\r\n",
    "(train_images, train_labels), (test_images, test_labels) = load_data_tensorflow()\r\n",
    "\r\n",
    "# Converting the training and testing images and labels to numpy arrays\r\n",
    "train_images = np.array(train_images)\r\n",
    "train_labels = np.array(train_labels)\r\n",
    "\r\n",
    "test_images = np.array(test_images)\r\n",
    "test_labels = np.array(test_labels)\r\n",
    "\r\n",
    "# Scaling the values of the images pixels to 0-1 to make the computation easier for our model\r\n",
    "train_images, test_images = train_images / 255, test_images / 255\r\n",
    "\r\n",
    "# Randomizing the training data\r\n",
    "train_images, train_labels = shuffle(train_images, train_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(\"Evaluating TF model...\")\r\n",
    "\r\n",
    "model = Sequential([\r\n",
    "    Conv2D(filters=32, activation='relu', input_shape=(img, img, 3), padding='same', kernel_size=(3, 3)),\r\n",
    "    Conv2D(filters=32, activation='relu', padding='same', kernel_size=(3, 3)),\r\n",
    "    MaxPool2D(pool_size=(2, 2)),\r\n",
    "    Conv2D(filters=64, activation='relu', padding='same', kernel_size=(3, 3)),\r\n",
    "    Conv2D(filters=64, activation='relu', padding='same', kernel_size=(3, 3)),\r\n",
    "    MaxPool2D(pool_size=(2, 2)),\r\n",
    "    Conv2D(filters=128, activation='relu', padding='same', kernel_size=(3, 3)),\r\n",
    "    Conv2D(filters=128, activation='relu', padding='same', kernel_size=(3, 3)),\r\n",
    "    MaxPool2D(pool_size=(2, 2)),\r\n",
    "    Conv2D(filters=256, activation='relu', padding='same', kernel_size=(3, 3)),\r\n",
    "    Conv2D(filters=256, activation='relu', padding='same', kernel_size=(3, 3)),\r\n",
    "    MaxPool2D(pool_size=(2, 2)),\r\n",
    "    Conv2D(filters=256, activation='relu', padding='same', kernel_size=(3, 3)),\r\n",
    "    Conv2D(filters=256, activation='relu', padding='same', kernel_size=(3, 3)),\r\n",
    "    MaxPool2D(pool_size=(2, 2)),\r\n",
    "    Conv2D(filters=512, activation='relu', padding='same', kernel_size=(3, 3)),\r\n",
    "    Conv2D(filters=512, activation='relu', padding='same', kernel_size=(3, 3)),\r\n",
    "    MaxPool2D(pool_size=(2, 2)),\r\n",
    "    Conv2D(filters=512, activation='relu', padding='same', kernel_size=(3, 3)),\r\n",
    "    Conv2D(filters=512, activation='relu', padding='same', kernel_size=(3, 3)),\r\n",
    "    Flatten(),\r\n",
    "    Dense(units=4096, activation='relu'),\r\n",
    "    Dense(units=4096, activation='relu'),\r\n",
    "    Dense(units=1, activation='sigmoid'),\r\n",
    "])\r\n",
    "\r\n",
    "# Printing the model summary\r\n",
    "model.summary()\r\n",
    "\r\n",
    "# Saving the weights of the model\r\n",
    "model.save('model.h5')\r\n",
    "\r\n",
    "# Compiling the model. Specifying the optimizer to act on the data. \r\n",
    "# The loss function (which could also have been sparse_categorical_crossentropy),\r\n",
    "# since there are only two classes, I have used binary_crossentropy\r\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\r\n",
    "model.load_weights('model.h5')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(\"Predicting...\")\r\n",
    "y_pred = model.predict(test_images)\r\n",
    "\r\n",
    "# print('Done.')\r\n",
    "\r\n",
    "# Using the confusion matrix to print the accuracy of those predictions\r\n",
    "cm = confusion_matrix(y_true=test_labels, y_pred=np.round(y_pred))\r\n",
    "# print(f'\\nAccuracy Score based on Confusion Matrix (TF): {(cm.trace() / cm.sum()) * 100}%')\r\n",
    "\r\n",
    "scores = cross_val_score(model, train_images, train_labels, cv=10)\r\n",
    "print(f'TensorFlow Cross-validation score: {scores.mean()}')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}